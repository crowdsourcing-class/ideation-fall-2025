# Crowdsourcing Project Idea: PitchPeer

## Author
Emily Kang, emkang

## Problem Statement
Many early-stage founders struggle to validate whether their startup ideas solve real customer problems before investing significant time or money. Existing validation processes (surveys, focus groups, or expert feedback) are often expensive, biased, or too small-scale. This project aims to crowdsource diverse, honest feedback from potential users and peers to help founders quickly gauge real-world interest.

## Core Concept
**One-line pitch:** A crowdsourced platform where founders submit short startup pitches, and a global community provides feedback, votes, and conducts quick user interviews to validate ideas.

**Target users:** Early-stage startup founders, entrepreneurship students, incubators, and accelerators seeking fast, unbiased market validation.

**The crowd:** Volunteers, startup enthusiasts, potential users, and entrepreneurship community members recruited via social media and online startup forums.

**The task:** Crowd participants will read a short startup pitch, answer structured feedback questions (e.g., “Would you use this?” “What’s unclear?”), optionally volunteer for short user interviews, and vote on perceived viability and interest.

## Key Features
1. Founder submission portal for 2-minute startup pitches.
2. Feedback interface with guided questions and upvote/downvote mechanisms.
3. Optional matching system for short video interviews between founders and interested crowd reviewers.

## Feasibility Check
**Data source:** User-submitted startup pitches collected via a Google Form or lightweight web app.

**Budget reality:** Platform could run on free/low-cost tools (Google Forms, Airtable, Notion, Zoom/Calendly) with <$500 used for small incentives or ad promotion.

**Crowd size needed:** A few hundred participants (200–500) to ensure varied feedback and statistically meaningful votes.

**Quality control approach:** Use reputation scores for frequent, thoughtful reviewers; employ upvotes or flags to surface high-quality feedback.

## Technical Approach
**Human tasks:** Reading pitches, rating clarity and usefulness, identifying market fit signals, and providing qualitative feedback.

**Automated tasks:** Aggregating ratings, visualizing feedback summaries, and scheduling optional interviews via automation tools.

**Aggregation method:** Weighted averaging of ratings; NLP summarization of open-text feedback to highlight recurring themes.

## Prior Work
**Similar projects:** Product Hunt (crowdsourced product discovery): focuses on launched products, not early validation. Reddit r/startups: provides informal feedback, but lacks structured data and aggregation.

**Lessons from past course projects:** Projects with clear, structured tasks and lightweight data collection tend to scale better than open-ended feedback loops. Starting small and iterating helps maintain engagement and quality.

## Why This Could Work
This project is feasible because it leverages simple existing tools, requires minimal technical infrastructure, and taps into an active online startup community eager to give and receive feedback. The structured approach makes it easy to crowdsource useful, actionable data while maintaining quality within limited resources and timeframe.
