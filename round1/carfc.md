# Crowdsourcing Project Idea: **Crosswise**

## Author
Caroline Cummings
carfc

## Problem Statement
The NYT crossword recently created a paywall for their daily mini crossword puzzle, leading to many upset users. At the same time, users of crossword apps have untapped collective insight into what makes a clue clever, fair, or fun. This project bridges that gap by crowdsourcing human feedback and clue creation to improve LLM-generated crosswords.

## Core Concept
**One-line pitch:** A collaborative crossword app where players solve and rate LLM-generated clues, refining future clue generation through crowdsourced feedback

**Target users:** Casual crossword players, puzzle enthusiasts, and AI-curious users who enjoy contributing to improving word games.

**The crowd:** Volunteers who play the game through a web or mobile interface. The player base doubles as the data contributors.

**The task:** Players solve daily mini crosswords generated by an LLM (e.g., GPT-4o or Mistral). After each puzzle, they rate clue quality (funny, confusing, accurate) or suggest their own better clue. This feedback is stored, aggregated, and later used to fine-tune or re-rank future LLM clue generations.

## Key Features
1. Dynamic LLM-generated crosswords — small daily puzzles created automatically from a vocabulary dataset.
2. Player feedback loop — players rate or rewrite clues, producing human-validated training data.
3. Clue improvement engine — aggregate top-rated human clues to retrain or rank future LLM generations.

## Feasibility Check
**Data source:** User's answers and time-to-complete data. Feedback forms at the end of puzzles.

**Budget reality:** There will be costs of calling the LLM API.

**Crowd size needed:** I think it could work with only 10s of workers.

**Quality control approach:** The game should be motivation enough to make the users attempt to solve the game as efficiently as possible. 

## Technical Approach
**Human tasks:** The puzzle-solving will require thought and rating clues.

**Automated tasks:** The training of the model will be automated. Suggesting improved clues

**Aggregation method:** Weighted averaging of ratings and frequency-based consensus for rewritten clues.

## Prior Work
**Similar projects:** Foldit and Eterna both use games to help collect data on protein folding for researchers. This project adopts that “citizen collaboration” model for language and creativity instead of biology.

**Lessons from past course projects:** Previous projects demonstrated the importance of clear, lightweight user tasks and keeping users engaged. This system keeps gameplay fun while integrating meaningful data collection.

## Why This Could Work
[2-3 sentences on why this is viable within course constraints]
