# Crowdsourcing Project Idea: AgriAid

## Authors

**Original Author:** Nikki Liu, nikkiliu

**Contributor:** Jevin Ta, jevinta

## Problem Statement

Climate change and extreme weather events are increasing the frequency of crop failures worldwide, yet early detection of crop stress is still unreliable. Satellite imagery can monitor vegetation, but automated models often fail to distinguish between normal seasonal change and true distress. AgriAid aims to fill this gap by using human judgment to label subtle signs of crop stress from satellite images, helping researchers and NGOs build early-warning systems for food insecurity.

## Target Audience

End users: Agricultural researchers, environmental scientists, and humanitarian organizations monitoring crop health.
Crowd workers: Students, citizen scientists, sustainability volunteers, and individuals interested in climate data annotation.

## Description

AgriAid is a web-based platform where volunteers label pairs of satellite images showing agricultural regions at two points in time. Participants tag visible changes (healthy, stressed, bare) and identify possible causes like drought, flood, or pest damage. The system aggregates these human judgments to create high-confidence training data for crop-stress detection models and regional early-warning dashboards.

## Project Type

_Select the category that best fits your project:_

- [✓] Human computation algorithm
- [ ] Social science experiment with the crowd
- [ ] Tool for crowdsourcing (requesters or workers)
- [ ] Business idea using crowdsourcing
- [ ] Other: [specify]

## Key Features

_List 5-8 key features or capabilities of your system. Expand from Round 1:_

1. Interactive Image Labeling: Simple side-by-side “before vs. after” viewer for crop regions.
2. Multi-Class Tagging: Workers classify vegetation health (healthy, stressed, bare) and likely stress cause.
3. Confidence Ratings: Users rate how confident they are in each label (1–5).
4. Crowd Consensus Dashboard: Aggregated results visualized as crop-stress heatmaps.
5. Quality Verification Layer: Redundant labeling with disagreement tracking.
6. Progress Gamification: Leaderboards and badges for top contributors (“Eco Analyst,” “Data Hero”).
7. [Feature 7 - optional]
8. [Feature 8 - optional]

## Feasibility: Crowd & Resources

**Where will your crowd workers come from?**
[E.g., MTurk, volunteers, social media users, students, etc.]

**What will they provide?**
[E.g., labels, votes, creative content, transcriptions, etc.]

**What skills do they need?**
[E.g., bilingual, domain expertise, basic internet skills, etc.]

**Do skills vary widely? How?**
[Explain if some workers will be better than others and why]

**How will you incentivize participation?**
[E.g., payment, gamification, intrinsic motivation, course credit. Be specific about your incentive design.]

**How much will it cost?**
[Provide rough cost estimates: $/task, $/worker, total budget needed]

**Where will your data come from?**
[E.g., public datasets, user-generated, scraped data, APIs, etc.]

**How many crowd workers do you need?**
[Estimate the scale: 10s, 100s, 1000s?]

## Technical Approach

**What are the main steps/components in your system?**

1. [Step 1: e.g., "User uploads image"]
2. [Step 2: e.g., "System segments image into regions"]
3. [Step 3: e.g., "Crowd labels each region"]
4. [Step 4: e.g., "System aggregates labels via majority vote"]
5. [Step 5: e.g., "ML model is trained on labeled data"]

**What parts are done by the crowd vs. automated?**
[Clearly distinguish human tasks from computational tasks]

**What technologies/tools will you use?**
[E.g., Python, React, AWS, specific ML libraries, MTurk API, etc.]

**How will you aggregate results from the crowd?**
[E.g., majority voting, weighted averaging, expert adjudication, ML filtering, etc.]

## Quality Control

**How will you ensure quality of crowd contributions?**

[Describe your quality control mechanisms in detail]

**Specific quality control methods:**
- [ ] Gold standard questions (test questions with known answers)
- [ ] Majority voting across multiple workers
- [ ] Expert review or verification
- [ ] Attention checks or trap questions
- [ ] Reputation/qualification systems
- [ ] Statistical outlier detection
- [ ] Other: [specify]

## Evaluation & Success Metrics

**How will you know if your project succeeds?**
[Describe specific metrics or evaluation criteria]

**What would success look like quantitatively?**
[E.g., "90% accuracy," "100 users in first week," "$X cost per task"]

## Challenges & Mitigation Strategies

_For each challenge, propose a mitigation strategy:_

**Challenge 1:** [Describe challenge]
**Mitigation:** [How will you address it?]

**Challenge 2:** [Describe challenge]
**Mitigation:** [How will you address it?]

**Challenge 3:** [Describe challenge]
**Mitigation:** [How will you address it?]

## Prior Work

_Are there similar projects or systems? How is yours different?_

[Describe similar work and what makes your approach unique or improved]

**Specific related projects:**
- [Related Project 1]: [How it relates and how you differ]
- [Related Project 2]: [How it relates and how you differ]

## Discussion Notes from Round 2

**What did you agree on?**
[Key points of agreement between partners]

**What concerns or pushback emerged?**
[Questions, challenges, or areas of uncertainty discussed]

**Why is this idea promising?**
[Explain why you chose to develop this idea over the alternative]

**What makes this sustainable and feasible?**
[Specific reasons this can work within course constraints]

## Additional Notes

_Any other relevant information, inspirations, or considerations?_

[Optional: Add any additional context, ideas, or references]
