# Fun Facts

## Team Members
- Tuneer Roy (tuneer@seas.upenn.edu)
- Javier Farach (jfarach@seas.upenn.edu)
- Julia Fremberg (jfrem@seas.upenn.edu)
- Adam Thomson (adamt38@seas.upenn.edu)

## Project Resources
- **Video Presentation**: https://drive.google.com/file/d/12DfO8wqRZ8sLY5yeh4FKDTOa7szjGgH7/view
- **Website**: Available (URL not specified in feedback)
- **Report**: 296789914_Fun_Facts.pdf

## Project Synopsis
Fun Facts is a gamified platform where users can submit fun facts and the crowd votes on whether each submission is fact or fiction. The project provides an edutainment experience combining knowledge sharing with gamification elements including leaderboards. It offers dual entry points through both a web application and MTurk, providing different incentive models.

## How It Uses the Crowd
The crowd contributes in multiple ways:
- **Content Creation**: Users submit fun facts to the platform
- **Verification**: Crowd votes on whether submissions are fact or fiction
- **Moderation**: Community helps maintain content quality
- **Engagement**: Gamification through leaderboards drives participation
- **Dual Crowds**: Both MTurk workers (paid) and web users (gamified) participate

## Strengths
- **Clear Motivation**: Well-defined market position in edutainment space
- **Multiple Incentive Models**: Successfully implements both gamification (web app) and payment (MTurk)
- **Good Differentiation**: Clear distinction from existing competitors
- **Strong Analysis**: Interesting findings about crowd behavior (MTurk workers showed 63:37 fact/myth ratio vs Penn students' 45:55)
- **Technical Implementation**: Clean, functional website with account creation
- **Thoughtful Ethics Discussion**: Recognized issues with binary fact/fiction labeling potentially polarizing complex issues

## Weaknesses
- **Limited User Feedback**: No indication of whether answers were correct or visualization of aggregated responses
- **Functionality Gaps**: Website lacks features for end-users to see results or learn from submissions
- **Ground Truth Challenge**: Difficulty in establishing what constitutes "fact" vs "fiction"
- **Binary Limitations**: Oversimplification of complex issues into fact/fiction dichotomy
- **Scalability Concerns**: Questions about how moderation would scale with growth

## Course Concept Alignment
The project demonstrates:
- **High understanding** of crowd dynamics, ethics, aggregation, and scaling
- **Adequate understanding** of incentives, skills, and quality control
- Strong grasp of using crowds for subjective judgments
- Good cost analysis for scaling considerations

## Recommendations for Improvement
1. Implement feedback mechanism showing correct answers to users
2. Add visualization of aggregated crowd responses
3. Consider adding nuance beyond binary fact/fiction (e.g., "partially true," "context-dependent")
4. Develop more sophisticated ground truth verification methods
5. Integrate educational elements showing why facts are true/false
6. Create category-based organization for different types of facts
7. Implement more robust quality control for controversial topics