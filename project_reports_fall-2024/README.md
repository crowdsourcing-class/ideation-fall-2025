# NETS 2130 Crowdsourcing - Final Project Reports Summary

## Overview
This document contains comprehensive reports for all final projects submitted for the NETS 2130 Crowdsourcing & Human Computation course. Each report synthesizes instructor feedback, evaluates how well projects illustrate course concepts, and provides recommendations for improvement.

## Projects Evaluated

### Showcase-Worthy Projects (Marked "Yes" by Instructors)
1. **DataLabeler** - Dense image captioning for AI training (2 Yes, 0 No)
2. **StudySphere** - Collaborative study materials platform (2 Yes, 1 No)
3. **Loopify** - Music vibe-based playlist creation (2 Yes, 0 No)
4. **MoralMap** - Moral decision data collection (1 Yes, 0 No)
5. **LingoLoop** - Generational slang definitions platform (1 Yes, 1 No - Mixed)

### Non-Showcase Projects
6. **PixelPatchwork** - Collaborative AI image editing (0 Yes, 2 No)
7. **Fun Facts** - Gamified fact/fiction voting (0 Yes, 2 No)
8. **Memory Mosaic** - Collaborative event photo sharing (0 Yes, 2 No)
9. **WeightIn** - Anonymous polling platform (0 Yes, 2 No)
10. **Frontline** - AI business idea evaluation (0 Yes, 2 No - Failed project)

## Key Themes Across Projects

### Common Strengths
- Strong technical implementations with functional web applications
- Good integration of APIs and external services
- Clear understanding of basic crowdsourcing concepts
- Professional UI/UX design in most projects

### Common Weaknesses
- Weak incentive design (over-reliance on payment or mandatory participation)
- Limited differentiation from existing platforms
- Insufficient real-world testing with actual crowds
- Missing sustainability plans post-course

### Course Concept Understanding
Most projects demonstrated:
- **High understanding**: Crowd dynamics, quality control, aggregation
- **Adequate understanding**: Ethics, skills, scaling
- **Weak understanding**: Incentive design, long-term sustainability

## Recommendations for Future Cohorts
1. Focus on sustainable incentive mechanisms beyond the course
2. Ensure clear differentiation from existing solutions
3. Test with real crowds in realistic conditions
4. Consider long-term viability and maintenance costs
5. Balance technical implementation with crowdsourcing innovation

## File Organization
- `01_PixelPatchwork.md` - Collaborative AI image editing
- `02_Fun_Facts.md` - Fact/fiction voting platform
- `03_Memory_Mosaic.md` - Event photo sharing
- `04_DataLabeler.md` - AI training data collection
- `05_LingoLoop.md` - Generational slang definitions
- `06_WeightIn.md` - Anonymous polling
- `07_StudySphere.md` - Study materials platform
- `08_Loopify.md` - Music playlist curation
- `09_MoralMap.md` - Moral decision mapping
- `10_Frontline.md` - AI business idea evaluation

## Usage
These reports are intended to:
- Provide comprehensive feedback to student teams
- Identify exemplary projects for future course iterations
- Document lessons learned for course improvement
- Serve as reference for crowdsourcing project design