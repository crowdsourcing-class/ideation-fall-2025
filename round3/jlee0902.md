# Crowdsourcing Project Idea: TerraTruth

## Authors

**Original Author:** Alexander Mehta, amehta26

**Round 2 Contributors:** Alexander Mehta, amehta26; Brandon Yan, bdonyan

**Round 3 Contributors:** Alexander Mehta, amehta26; Brandon Yan, bdonyan; Josh Lee, jlee0902; Andrew Park, andrewpp

## Problem Statement

Non-profits and humanitarian groups lack manpower to analyze satellite imagery fast enough to track crises or environmental change. This delays aid and response. TerraTruth mobilizes trained volunteers to perform scalable, accurate, real-time image analysis for social good.

## Target Audience

**End Users:** Non-profits, journalists, researchers using the aggregated reports

**Crowd Workers:** Global volunteers passionate about humanitarian/environmental causes

**Stakeholders:** Imagery providers, academic labs, NGOs, donors

## Description

TerraTruth is a web-based crowdsourcing platform that connects non-profit organizations with a global pool of trained volunteers. Volunteers select "missions" aligned with causes they care about (e.g., "Estimate deforestation in the Amazon," "Identify damaged infrastructure in a conflict zone"). The platform guides them through analyzing satellite imagery, and their combined inputs are aggregated into verified reports that are provided directly to partner organizations to drive awareness and action.

## Project Type

_Select the primary category:_

- [X] Human computation algorithm
- [ ] Social science experiment with the crowd
- [ ] Tool for crowdsourcing (requesters or workers)
- [ ] Business idea using crowdsourcing
- [ ] Other: [specify]

## Key Features

_List 8-10 key features or capabilities of your system:_

1. **Mission-Based Cause Selection**: Users can browse and join analysis "missions" sponsored by partner non-profits, filtered by cause (e.g., Humanitarian, Environmental, Conflict).
2. **Reputation & Skill System**: Workers earn accuracy scores and badges based on their performance against gold-standard data, unlocking more complex tasks as their skill is proven.
3. **Weighted Aggregation Engine**: Responses are aggregated using a weighted average based on the analyst's proven accuracy score, improving reliability and weeding out low-quality data.
4. **Impact Visibility Dashboard**: A dedicated section showing real-time contribution statistics and publishing updates from non-profits on how the data was used (e.g., "Your analysis was featured in this report...").
5. **Sensitive Content Filtering**: Users can opt-out of viewing potentially graphic or disturbing imagery, ensuring a safe and comfortable volunteer experience.
6. **Progressive Training Module**: New users are guided through tutorials and "training missions" with known answers to build their skills before working on live, unverified data.
7. **Partner Data Portal**: Verified and aggregated data (e.g., counts, affected area polygons, key findings) is made available to partner organizations via a secure API or data export dashboard.
8. **AI-Powered Task Pre-Filtering**: An integrated AI module scans incoming satellite imagery to triage tasks by difficulty (e.g., flagging clear vs. ambiguous scenes). Easier tasks are assigned to new users, while complex ones go to experienced volunteers. This human-AI collaboration boosts overall efficiency and ensures appropriate task-to-skill matching.
9. **Gamified Achievements & Leaderboards**: To sustain motivation, volunteers earn badges, streak bonuses, and leaderboard ranks for completing missions accurately and consistently. Seasonal challenges and team missions add social engagement, encouraging continued participation and friendly competition among users.
10. **Community Collaboration Forum**: A built-in discussion board and chat system allow volunteers to share insights, ask questions, and learn from expert moderators. This fosters a sense of belonging and enables peer-to-peer learning — turning TerraTruth from a solo annotation tool into a global volunteer community.

## Feasibility: Crowd & Resources

**Where will your crowd workers come from?**
Primarily **volunteers** recruited through partner non-profits, university groups, social media campaigns (targeting users interested in humanitarian/environmental causes), and online volunteer portals.

**What will they provide?**
Image annotations (e.g., bounding boxes), object counts (e.g., buildings, vehicles, tents), damage assessments, and land-use classifications (e.g., "deforested," "flooded," "new construction").

**What skills do they need?**
Basic visual pattern recognition and computer literacy. No domain expertise is required for entry-level tasks, as the platform will provide task-specific training.

**Do skills vary widely? How?**
Yes. Some users will naturally be better at
spotting subtle details or more consistent in their counting. Skill will be measured by their accuracy against "gold standard" images, and this will form their reputation score.

**How will you incentivize participation?**
Primarily **intrinsic motivation** (contributing to a cause they believe in). This is supported by:
1.  **Gamification:** Earning badges for achievements, rising on leaderboards, and "leveling up" to unlock harder tasks.
2.  **Impact Visibility:** Regularly showing users how their specific contributions were used by partner organizations.
3.  **Community:** Building a community forum where users can discuss missions and feel part of a team.

**Budget & Cost Analysis**
TerraTruth operates as a non-profit, volunteer-driven platform. The main cost components are:  
- **Cloud hosting and storage:** approximately \$200 for AWS or GCP (EC2, S3, RDS).  
- **Web infrastructure and domain services:** approximately \$50.  
- **Promotional design and outreach:** approximately \$150 for ad materials.  
- **Volunteer rewards and recognition items:** approximately \$100.  

The **total estimated budget** is **≈ \$500**, aligning with the course constraint.  
Since all labor and data sources are free, the **estimated cost per task** is less than \$0.01, and **cost per worker** is \$0 (volunteers).  
If the pilot mission contains about **10,000 image tasks**, then  
`total_cost ≈ $500` → `cost_per_task ≈ $500 / 10,000 = $0.05`.  
This is sustainable for small-scale deployment and easily extendable via grant funding.

**Where will your data come from?**
Data will be sourced from **public satellite imagery** (Sentinel-2, Landsat 8), **open humanitarian datasets** (e.g., xBD Dataset by DIUx + Maxar), and **partner organizations** such as Amnesty International or Planet Labs via non-profit data agreements. These sources ensure both ethical use and cost-free access to high-quality imagery.

**Scale Requirements**
A minimum of **50 active volunteers** is sufficient to run the first pilot mission (≈1,000 images).  
For full platform functionality, TerraTruth targets around **500 active users** contributing across multiple missions.  
The architecture is designed to scale naturally to **5,000 + global volunteers**, as tasks are independent and can be distributed seamlessly through the task-queue system.

## Technical Architecture

### System Components
1. **Frontend Web Interface (React):**  
   Provides an interactive dashboard for volunteers to select missions, annotate satellite images, view their progress, and track impact metrics. Includes components for authentication, annotation tools, and leaderboards.

2. **Backend API (Python Flask/Django):**  
   Handles mission creation, task queuing, user authentication, reputation tracking, and data aggregation. Manages communication between the frontend and the database.

3. **Database (PostgreSQL with PostGIS):**  
   Stores user profiles, image metadata, annotation data, reputation scores, and mission configurations. PostGIS enables efficient geographic queries and spatial data storage.

4. **Task Management & Quality Control Engine:**  
   Automatically segments incoming imagery into micro-tasks, mixes in gold-standard images, assigns tasks to users, and evaluates responses for quality and consistency.

5. **Aggregation & Reporting Module:**  
   Computes weighted aggregation results, detects outliers, and generates visual reports and data exports for partner organizations. Also supports API endpoints for NGO data retrieval.

6. **Hosting & Infrastructure:**  
   Hosted on AWS or GCP with EC2 instances for backend servers, S3 for storing image tiles, and RDS for databases. Containerization via Docker ensures scalable deployment.

---

### Detailed Workflow
**Step 1:** A partner organization uploads a set of satellite images and defines a mission (e.g., *“Identify flood-affected regions in Bangladesh”*).  
**Step 2:** The system preprocesses and segments each image into smaller tiles (e.g., 512×512 px). A small percentage of these are tagged as **gold-standard** tasks with known answers.  
**Step 3:** Tasks are queued in the backend and distributed to available volunteers through the web interface.  
**Step 4:** Volunteers annotate or classify each image tile (e.g., damaged / intact / flooded). Their annotations are sent to the backend and logged in the database.  
**Step 5:** Each image tile is reviewed by multiple workers (typically 3–5). The **aggregation engine** combines responses using weighted averaging or majority voting, weighted by user reputation scores.  
**Step 6:** Outliers and low-confidence results are flagged automatically for re-annotation or expert review.  
**Step 7:** Once all tasks are complete, the system compiles aggregated results into summary maps and reports, which are made available to the partner NGO via the **Partner Data Portal**.  
**Step 8:** Worker reputation scores are updated based on their performance on gold-standard images, feeding back into the quality-control loop.

---

### Human vs Automated Tasks
- **Human Tasks (Crowd):**  
  Image interpretation and visual reasoning tasks such as identifying damaged buildings, classifying terrain, and estimating object counts—activities where human perception outperforms automated models.

- **Automated Tasks (System):**  
  Task segmentation, distribution, data storage, performance scoring, aggregation, quality checks, and visualization. The system handles repetitive computation, ensuring scalability and consistency.

---

### Technologies & Tools
- **Frontend:** React + Tailwind CSS for dynamic, responsive UI and annotation tools.  
- **Backend:** Python (Flask or Django) for APIs, mission management, and aggregation logic.  
- **Database:** PostgreSQL + PostGIS for structured and spatial data storage.  
- **Hosting:** AWS (EC2 for backend, S3 for image storage, RDS for DB).  
- **Version Control / Deployment:** GitHub + Docker for CI/CD and containerized scaling.  
- **Visualization:** Mapbox GL JS for displaying geographic results to partners and volunteers.

---

### Aggregation Method
For **numerical tasks** (e.g., counting objects), TerraTruth applies a **weighted average**:  
`final_value = Σ(wᵢ × aᵢ) / Σ(wᵢ)`  
where *aᵢ* is a worker’s annotation and *wᵢ* is their reputation-based weight.  

For **categorical tasks** (e.g., “damaged / undamaged”), the system uses a **weighted majority vote**, where higher-reputation users’ labels have greater influence. Reputation scores are continuously updated using performance on gold-standard images and agreement with aggregated consensus.

---

### Why This Architecture
This design balances human intuition with automated scalability. Volunteers perform perceptual analysis, while the backend automates task routing, validation, and aggregation. The modular architecture enables easy extension to new missions and seamless scaling as the crowd size grows.

## Quality Control

**Quality Control Strategy:**
We want to ensure high accuracy and reliability in satellite image analysis while maintaining volunteer engagement concurrently. The strategy blends human validation, statistical weighting, and automated consistency checks. By layering multiple QC mechanisms such as gold standard testing, redundancy, and reputation scoring, the system ensures that individual worker errors are minimized, and aggregated results remain trustworthy.
We believe this hybrid approach allows TerraTruth to operate effectively at scale, combining the sensitivity of human perception with algorithmic rigor.

**Specific QC Mechanisms:**
- [X] Gold standard questions (test questions with known answers)
  - Approximately 10-15% of all tasks are gold-standard images with pre-verified annotations.
  - These are randomly interspersed among regular tasks so that workers cannot distinguish them.
  - Workers' accuracy on these tasks directly affects their reputation score and determines access to higher-level missions.
  - If a worker repeatedly fails gold-standard checks (<70% accuracy after 20 tasks), they are temporarily restricted to training missions for retraining before resuming live tasks.
- [X] Majority voting across multiple workers
  - Each image tile is annotated by 3-5 independent workers.
  - For categorical labels (e.g., "damaged," "undamaged"), TerraTruth uses weighted majority voting, giving more influence to workers with higher reputation scores.
  - In case of a tie or low-confidence consensus (agreement <60%), the task is requeued for additional annotations or flagged for expert review.
- [X] Expert review or verification
  - Expert reviewers include academic partners, NGO analysts, and trained moderators.
  - Approximately 5-10% of aggregated results are randomly sampled for expert validation, focusing on borderline or low-confidence images.
  - Experts' corrections feed back into the gold-standard dataset, improving future worker training and system calibration.
- [ ] Attention checks or trap questions
  - _Details: [What types? How often? Consequences?]_
- [ ] Reputation/qualification systems
  - _Details: [How do workers build reputation? How is it used?]_
- [X] Statistical outlier detection
  - The backend continuously computes z-scores and inter-worker agreement variance to detect anomalous annotations.
  - Outliers (e.g., a worker labeling 90% of tiles as "damaged") are flagged for review.
  - If confirmed as low-quality, their contributions are excluded from aggregation, and the user is notified for retraining.
- [X] Redundancy and agreement metrics
  - Minimum redundancy: 3 workers per task; optimal redundancy: 5 for complex imagery.
  - Agreement is tracked using Cohen's kappa and Krippendorff's alpha to measure inter-rater reliability.
  - Tasks with low agreement (<0.5 kappa) are automatically reissued for additional reviews or expert inspection.
- [ ] Other: [specify]
  - _Details: [Explain custom QC approaches]_

**Handling Low-Quality Work:**
When quality issues are detected:

* **Rejection**: Low-confidence or clearly incorrect annotations are discarded automatically.
* **Re-tasking**: Flagged images are reassigned to new workers or experts for re-evaluation.
* **Retraining prompts**: Workers are provided with targeted examples showing their mistakes, enabling learning and gradual improvement rather than outright exclusion.

This multi-tiered QC framework ensures TerraTruth maintains scientific-grade data quality while sustaining volunteer engagement and fairness.

## Evaluation & Success Metrics

**Primary Success Metrics:**

1. **Accuracy:**  Achieve ≥95% aggregated accuracy on gold-standard satellite imagery tasks, as measured by the proportion of crowd-verified annotations that match expert-verified labels.
2. **Engagement:**  Reach ≥500 active monthly volunteers within the first year, with ≥50% returning users across multiple missions. Measured through login frequency and mission participation logs.
3. **Throughput:**  Process at least 10,000 annotated images per month after launch, verified by database task completion rates and aggregation logs.


**Evaluation Methodology:**

Evaluation will combine **quantitative data analysis** and **partner feedback**:

- **Data Accuracy Evaluation:**  
  Each mission will include 10–15% gold-standard images. Crowd performance will be compared against these expert-labeled samples to compute precision, recall, and F1 score.  
  Formula for aggregated accuracy:  
  `accuracy = (correct_annotations / total_gold_standard_annotations) × 100`

- **Volunteer Engagement Tracking:**  
  Engagement will be measured via backend logs (number of unique active users per week, total annotations per user, and retention rates). Periodic surveys will capture qualitative motivation and satisfaction.

- **System Throughput Monitoring:**  
  The backend automatically tracks number of images processed, average task completion time, and server uptime using monitoring tools (e.g., AWS CloudWatch).

- **Impact Assessment:**  
  NGO partners will be surveyed quarterly to evaluate how TerraTruth’s insights contributed to decision-making, awareness, or reports. Direct citations or attributions will be logged as concrete impact metrics.


**Baseline Comparisons:**
- **Baseline 1 — Expert Annotation Only:**  
  Compare TerraTruth’s aggregated accuracy against expert-only analysis of the same dataset. The goal is to achieve within ±5% accuracy of expert performance at a fraction of the cost.

- **Baseline 2 — Simple Majority Voting:**  
  Compare weighted aggregation results against unweighted majority voting to demonstrate the advantage of incorporating user reputation and dynamic weighting.

- **Improvement Measurement:**  
  Improvement (%) = `((weighted_accuracy - baseline_accuracy) / baseline_accuracy) × 100`


**Success Criteria:**

- **Minimum Viable Success:**  
  - 80% aggregated accuracy  
  - 100+ active volunteers  
  - 2,000 images processed in pilot phase  

- **Target Success:**  
  - 95% accuracy  
  - 500+ active monthly volunteers  
  - 10,000+ images processed monthly  
  - 2 NGO partners citing results  

- **Stretch Success:**  
  - 97%+ accuracy  
  - 1,000+ active volunteers globally  
  - 3+ NGO partnerships with long-term adoption  
  - Public recognition in major environmental or humanitarian reports  

## Challenges & Mitigation Strategies

**Challenge 1:** Sourcing consistent, high-quality, and *timely* satellite imagery for free.
* **Why it's a risk:** Without a steady stream of relevant, recent imagery, the platform has no "missions" for volunteers. This stalls all user activity, prevents us from building an engaged community, and makes it impossible to demonstrate value to potential non-profit partners or data providers.
* **Mitigation strategy:** Start by focusing on publicly available data (e.g., Sentinel, Landsat) to build the platform and initial user base. Use this demonstrated capability and user-base size to then secure formal partnerships with providers like Planet Labs and Maxar for their non-profit/humanitarian data streams.
* **Backup plan:** If formal partnerships for timely data fail to materialize, we will pivot to being a "value-add" platform *exclusively* for public data. We would focus on building best-in-class analysis tools for Landsat/Sentinel data, targeting researchers and smaller NGOs who lack the technical expertise to process this data themselves.

**Challenge 2:** Maintaining volunteer engagement and preventing burnout.
* **Why it's a risk:** Our entire model relies on unpaid, intrinsically motivated volunteers. If the work feels unappreciated, boring, or futile (especially for difficult tasks like conflict analysis), users will abandon the platform. This leads to high user churn, incomplete missions, and an inability to deliver timely reports to NGOs.
* **Mitigation strategy:** Implement strong gamification (Key Feature 9) and a best-in-class "Impact Dashboard" (Key Feature 4). We must work *very* closely with non-profit partners to get regular, tangible updates and human stories to share with volunteers to close the feedback loop and show them their work matters.
* **Backup plan:** If individual gamification is insufficient, we will pivot to a "team-based" model. We will partner with university classes, student clubs, and corporate social responsibility (CSR) programs to run time-boxed "analysis drives" or "mapathons," leveraging existing social structures for engagement.

**Challenge 3:** Ensuring data accuracy from untrained volunteers for complex analysis.
* **Why it's a risk:** If our aggregated reports are inaccurate (e.g., "is this building damaged by conflict or just under construction?"), we lose the trust of our non-profit partners. Providing bad data is *worse* than providing no data, as it could lead to misallocation of aid or flawed reporting, destroying the platform's reputation.
* **Mitigation strategy:** Use a **progressive task model** and **reputation system** (Key Features 2 & 6). New users *must* pass simple "training" missions with gold-standard data. As their accuracy score improves, they "unlock" more complex tasks. Critical analysis is only done by proven, high-skill users and aggregated using weighted voting (Aggregation Method).
* **Backup plan:** If the progressive model still yields low-quality data for complex tasks, we will restrict those tasks entirely. The platform would pivot to focus *only* on simple, high-volume tasks (e.g., "find all tents," "map all roads") where simple aggregation is highly reliable, leaving complex analysis to experts.

**Challenge 4:** Misuse of data or negative psychological impact on volunteers.
* **Why it's a risk:** The platform analyzes sensitive data, including conflict zones and humanitarian disasters. This presents two risks: 1) Malicious actors could try to access the aggregated data for targeting. 2) Volunteers may be exposed to graphic or upsetting imagery (e.g., destroyed homes), causing psychological distress and burnout (related to Challenge 2).
* **Mitigation strategy:** 1) Data Access: Implement strict partner vetting. Only verified non-profits (End Users) can access raw data via the Partner Data Portal (Key Feature 7). Public reports will be aggregated or time-delayed. 2) User Well-being: Implement the "Sensitive Content Filtering" (Key Feature 5) as an opt-out for all users, and provide clear content warnings before users begin a potentially distressing mission.
* **Backup plan:** If data is misused or volunteers report significant distress, we will pivot to *only* non-humanitarian missions. The platform's focus would shift entirely to environmental monitoring (e.g., deforestation, glacial melt, agriculture), which has a near-zero risk of misuse or psychological harm.

## Prior Work & Related Projects

**Similar Existing Systems:**

1.  **[Project/System 1 Name]** Zooniverse
    * **Description:** The largest and most popular citizen science platform, where volunteers assist academic researchers in analyzing large datasets (e.g., classifying galaxies, identifying animals).
    * **Similarities:** Uses a volunteer-based, web-platform model for large-scale data analysis. Successfully gamifies and motivates a large volunteer crowd.
    * **Differences:** Zooniverse is focused on general academic science (like galaxy classification or ecology) with a goal of scientific discovery. TerraTruth is focused specifically on **time-sensitive humanitarian and environmental action**. Our "why" is activism and real-time impact, not just discovery.
    * **Citation/Link:** `https://www.zooniverse.org/`

2.  **[Project/System 2 Name]** Amnesty International's "Decoders"
    * **Description:** A past series of successful projects by Amnesty International that mobilized "digital volunteers" to analyze satellite imagery and other data to track human rights abuses (e.g., in Darfur, Sudan).
    * **Similarities:** Directly applied volunteer-based image analysis to a humanitarian cause (conflict/human rights). Proved this model can be effective.
    * **Differences:** "Decoders" consisted of single, short-term campaigns run *by* one specific NGO. TerraTruth aims to be a **permanent, scalable platform** that serves *multiple* organizations simultaneously. We are building the reusable *tool* for all NGOs.
    * **Citation/Link:** `https://www.amnesty.org/en/latest/campaigns/2016/09/digital-volunteers-help-uncover-darfur-abuses/` (Example article)

3.  **[Project/System 3 Name - optional]** Commercial Data Labeling Platforms (e.g., Amazon Mechanical Turk or NETS Project Datalabeler)
    * **Description:** For-profit platforms (like MTurk, Scale AI, Labelbox) that pay crowd workers to perform micro-tasks, often to generate training data for machine learning models.
    * **Similarities:** The core task (image annotation, classification) and technical architecture (task queuing, aggregation) are very similar.
    * **Differences:** These platforms differ entirely in their **goal** and **incentive**. They are commercial and rely on extrinsic (paid) motivation. TerraTruth is a non-profit and is built entirely on **intrinsic motivation** (contributing to a cause) for the goal of direct social good, not training AI.
    * **Citation/Link:** `https://www.mturk.com/`

**Lessons from Past Course Projects:**
We learned from projects like Dataloader than motivation is the most important factor, attracting users and getting them to keep using your platform is very difficult. Therefore, we focused on our platform having multiple different causes and letting users chose the one that appealed to them.

## Ethical Considerations

**Potential Ethical Issues:**
- **Exposure to Sensitive or Distressing Content:**  
   Some satellite imagery may capture scenes of violence, destruction, or environmental disasters that could cause emotional distress for volunteers.
- **Data Privacy and Security:**  
   Although most data is public satellite imagery, partner organizations may occasionally upload proprietary or location-sensitive datasets that must be handled securely.
- **Fairness and Volunteer Exploitation Concerns:**  
   Because TerraTruth relies on unpaid volunteers, there is a risk of perceived exploitation or inequity if the system does not provide meaningful recognition or agency for contributors.


**Mitigation Strategies:**
- **Addressing Sensitive Content:**  
  Implement an **opt-out feature** and **content filtering system** to flag potentially disturbing imagery before assignment. Volunteers will be informed of potential exposure risks and can choose missions by comfort level. Resources for mental health support or reflection will be provided for heavy humanitarian content.
- **Ensuring Data Privacy and Security:**  
  All uploaded imagery and annotations will be stored in encrypted databases (e.g., AWS S3 with restricted access). Only aggregated, non-identifiable outputs are shared externally. Partner organizations will sign data-sharing agreements that comply with GDPR and humanitarian data ethics guidelines.
- **Maintaining Fairness and Volunteer Respect:**  
  The platform clearly communicates its **non-profit, impact-driven mission**, emphasizing that all contributions directly aid real-world humanitarian efforts. Volunteers receive **public recognition**, **leaderboard rankings**, and **digital certificates** to ensure their efforts are acknowledged. Participation is entirely optional and transparent in purpose.

**IRB Considerations:**
TerraTruth **does not require IRB approval** because it does not involve human subjects research or collect personally identifiable data from participants beyond basic account credentials (e.g., email for login). All data analyzed are either publicly available satellite images or organizational uploads with explicit consent. However, ethical design standards (informed consent, content warnings, and data protection) will be followed to align with research best practices.

## Business Viability (Optional)

_If your project has business potential, consider:_

**Revenue Model:**
[How could this make money?]

**Market Size:**
[Who would pay for this? How many potential customers?]

**Competitive Advantage:**
[Why would people choose your solution?]

**Sustainability:**
[Can this continue after the course ends?]

## Steel-Man Discussion (Round 3)

**Idea A Steel-Man:**
We strengthened TerraTruth into a scalable platform that mobilizes trained volunteers for real-time satellite image analysis. By integrating AI pre-screening, a weighted reputation system, and mission-based task design, it became a professional yet feasible system for humanitarian and environmental monitoring. The final version balances social impact, technical credibility, and affordability, making it suitable for real-world deployment and academic validation alike.

**Idea B Steel-Man:**
We refined Dialect Atlas into a global platform for recording and comparing regional dialects. It evolved to include machine-assisted clustering, expert validation from linguists, and gamified participation to sustain engagement. The steel-manned version offered strong cultural value and data richness but lacked the same immediacy of humanitarian application as TerraTruth.

**Why We Chose This Idea:**
We chose TerraTruth because it provides a clearer pathway to measurable social impact, technical scalability, and long-term sustainability. It leverages existing open satellite datasets, aligns naturally with non-profit missions, and can realistically be built under the project’s resource constraints.

**What We Agreed On:**
All members agreed that the final idea should create social value, maintain rigorous data quality, and motivate sustained participation. We also shared the belief that combining human judgment with automated tools is essential for scalability and accuracy.

**What Concerns Emerged:**
Concerns focused on data ethics, volunteer retention, and maintaining consistent annotation quality at scale. We also noted the need to manage infrastructure costs efficiently and build trustworthy NGO partnerships for data use.

**Key Insights from Round 3 Discussion:**
We learned that strong narrative framing of presenting TerraTruth as a "volunteer-powered AI for social good" made the idea more compelling. Gamification emerged as key to worker retention, and integrating feedback loops into training enhanced both accuracy and engagement. Overall, our discussion clarified how human-AI collaboration could transform volunteer energy into reliable, actionable intelligence for real-world impact.

## Implementation Timeline (Optional)

_If you were to build this, what would the timeline look like?_

**Week 1-2:** [Phase 1 tasks]
**Week 3-4:** [Phase 2 tasks]
**Week 5-6:** [Phase 3 tasks]
**Week 7-8:** [Phase 4 tasks]

## Additional Notes

_Any other relevant information, inspirations, or considerations?_

[Optional: Add any additional context, ideas, or references]

## References & Inspiration

_Include any references, papers, articles, or sources that inspired or informed this project idea:_

1. Global Forest Watch: https://www.globalforestwatch.org/
2. Zooniverse: https://www.zooniverse.org/
3. DataLabeler Report: https://github.com/crowdsourcing-class/ideation-fall-2025/blob/main/project_reports_fall-2024/04_DataLabeler.pdf
